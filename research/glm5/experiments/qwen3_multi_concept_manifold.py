# -*- coding: utf-8 -*-
"""
Qwen3 ç¼–ç ç»“æ„å››ç»´åº¦æå–å™¨
=========================
ä» Qwen3-4B ä¸­æå–ç¼–ç ï¼ŒéªŒè¯å››ä¸ªå…³é”®æ•°å­¦ç‰¹æ€§ï¼š
  1. é«˜ç»´æŠ½è±¡ â€” è¯­ä¹‰æ”¶æ•›èƒ½åŠ›
  2. ä½ç»´ç²¾ç¡® â€” ç»†ç²’åº¦åŒºåˆ†èƒ½åŠ›
  3. ç‰¹å¼‚æ€§ â€” æ¦‚å¿µå­ç©ºé—´æ­£äº¤æ€§
  4. ç³»ç»Ÿæ€§ â€” ç±»æ¯”å…³ç³»ä¸€è‡´æ€§

è¾“å‡º: tempdata/qwen3_structure_report.json + 4 å¼ å¯è§†åŒ–å›¾
"""

import json
import os
import sys
import time

import matplotlib

matplotlib.use("Agg")  # æ— å¤´æ¨¡å¼ï¼Œå…¼å®¹æœåŠ¡å™¨
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity

# ============================================================
# ç¬¬é›¶éƒ¨åˆ†ï¼šæ¨¡å‹åŠ è½½ï¼ˆå¤ç”¨å·²éªŒè¯çš„ import_trace.py é€»è¾‘ï¼‰
# ============================================================

SNAPSHOT_PATH = r"D:\develop\model\hub\models--Qwen--Qwen3-4B\snapshots\1cfa9a7208912126459214e8b04321603b3df60c"

# ç¯å¢ƒå˜é‡
os.environ["HF_HOME"] = r"D:\develop\model"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TORCH_FORCE_WEIGHTS_ONLY_LOAD"] = "0"


def load_qwen3():
    """åŠ è½½ Qwen3-4B ä¸º HookedTransformer"""
    import transformers.configuration_utils as config_utils
    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

    from transformer_lens import HookedTransformer

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[*] åŠ è½½ Qwen3-4Bï¼Œè®¾å¤‡: {device}")
    print(f"    è·¯å¾„: {SNAPSHOT_PATH}")

    t0 = time.time()

    # æ­¥éª¤ 1: åœ¨ CPU ä¸ŠåŠ è½½ HF æ¨¡å‹ (HookedTransformer ä¼šè‡ªè¡Œå¤„ç†è®¾å¤‡è¿ç§»)
    hf_model = AutoModelForCausalLM.from_pretrained(
        SNAPSHOT_PATH, local_files_only=True, trust_remote_code=True,
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(
        SNAPSHOT_PATH, local_files_only=True, add_bos_token=False
    )

    # ä¿®å¤1: Qwen3 tokenizer ç¼ºå°‘ bos_token
    if tokenizer.bos_token is None:
        tokenizer.bos_token = tokenizer.eos_token
        tokenizer.bos_token_id = tokenizer.eos_token_id
        print(f"    [fix] è®¾ç½® bos_token = eos_token ({tokenizer.bos_token})")

    # ä¿®å¤2: Monkey-patch PretrainedConfig ä»¥ä¿®å¤ rope_theta
    _orig_getattr = config_utils.PretrainedConfig.__getattribute__

    def _patched_getattr(self, key):
        if key == "rope_theta":
            try:
                return _orig_getattr(self, key)
            except AttributeError:
                try:
                    rs = _orig_getattr(self, "rope_scaling")
                    if isinstance(rs, dict) and "rope_theta" in rs:
                        return rs["rope_theta"]
                except (AttributeError, TypeError):
                    pass
                return 1000000
        return _orig_getattr(self, key)

    config_utils.PretrainedConfig.__getattribute__ = _patched_getattr

    # ä¿®å¤3: Monkey-patch get_tokenizer_with_bos é¿å…é‡æ–°åŠ è½½ tokenizer
    import transformer_lens.utils as tl_utils
    _orig_get_tok_bos = tl_utils.get_tokenizer_with_bos

    def _patched_get_tok_bos(tok):
        # ç›´æ¥è¿”å›å·²ä¿®å¤çš„ tokenizerï¼Œé¿å…é‡æ–° from_pretrained
        return tok

    tl_utils.get_tokenizer_with_bos = _patched_get_tok_bos
    print("    [fix] å·² monkey-patch rope_theta + get_tokenizer_with_bos")

    try:
        model = HookedTransformer.from_pretrained(
            "Qwen/Qwen3-4B", hf_model=hf_model, device=device, tokenizer=tokenizer,
            fold_ln=False, center_writing_weights=False, center_unembed=False,
            dtype=torch.float16, default_prepend_bos=False
        )
    finally:
        config_utils.PretrainedConfig.__getattribute__ = _orig_getattr
        tl_utils.get_tokenizer_with_bos = _orig_get_tok_bos
        print("    [fix] å·²æ¢å¤æ‰€æœ‰ monkey-patch")

    model.eval()
    print(f"[+] æ¨¡å‹åŠ è½½å®Œæˆ ({time.time() - t0:.1f}s)")
    print(f"    å±‚æ•°: {model.cfg.n_layers}, ç»´åº¦: {model.cfg.d_model}")
    return model


def calculate_iou(set1, set2):
    intersection = len(set(set1).intersection(set(set2)))
    union = len(set(set1).union(set(set2)))
    return intersection / union if union > 0 else 0

def calculate_cosine_sim(v1, v2):
    v1_norm = v1 / (torch.norm(v1) + 1e-9)
    v2_norm = v2 / (torch.norm(v2) + 1e-9)
    return torch.dot(v1_norm, v2_norm).item()

def run_multi_concept_manifold():
    print("\nğŸŒŒ å¯åŠ¨ Qwen3 å¤šæ¦‚å¿µæµå½¢ç©ºé—´è§£æ...")
    model = load_qwen3()
    
    # å®šä¹‰å¤šä¸ªæ­£äº¤çš„æ¦‚å¿µç»´åº¦
    concepts = {
        "Capital": "The capital of France is",
        "Arithmetic": "The result of 2 + 3 is",
        "Color": "The color of the sky is",
        "Antonym": "The opposite of hot is",
        "Syntax": "He ran quickly across the",
        "Gender": "The king is man, the queen is"
    }
    
    target_layer = model.cfg.n_layers // 2 + 2 
    ablate_k = 50  # æå–æœ€é¡¶çº§çš„ 50 ä¸ªæ¿€æ´»ç»´åº¦
    
    results = {}
    top_indices_dict = {}
    full_vectors_dict = {}
    
    print(f"\nğŸ“¡ å¼€å§‹åœ¨ç¬¬ {target_layer} å±‚æµ‹ç»˜éšç©ºé—´...")
    
    for name, prompt in concepts.items():
        _, cache = model.run_with_cache(prompt)
        resid_post = cache[f"blocks.{target_layer}.hook_resid_post"][0, -1, :]
        full_vectors_dict[name] = resid_post
        
        # è·å– Top-K ç´¢å¼•
        top_indices = torch.topk(resid_post.abs(), ablate_k).indices.tolist()
        top_indices_dict[name] = top_indices
        print(f"  [{name}] å…±é”å®š {ablate_k} æ ¹ç‰¹å¾ä¸»åŠ›çº¤ç»´.")

    # 1. è®¡ç®—é‡å åº¦ (IoU) - éªŒè¯ç‰©ç†é€šé“çš„ç»å¯¹ç¨€ç–ä¸éš”ç¦»
    print("\nğŸ§¬ è®¡ç®—æ¦‚å¿µçº¤ç»´é—´çš„ Jaccard ç›¸ä¼¼ç³»æ•° (IoU):")
    iou_matrix = {}
    concept_names = list(concepts.keys())
    for i in range(len(concept_names)):
        iou_matrix[concept_names[i]] = {}
        for j in range(len(concept_names)):
            if i == j:
                iou_matrix[concept_names[i]][concept_names[j]] = 1.0
            else:
                iou = calculate_iou(top_indices_dict[concept_names[i]], top_indices_dict[concept_names[j]])
                iou_matrix[concept_names[i]][concept_names[j]] = round(iou, 4)
                if i < j:
                    print(f"  {concept_names[i]} vs {concept_names[j]} -> IoU: {iou:.4f}")

    # 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - éªŒè¯æ•´ä½“å‡ ä½•ç©ºé—´çš„æ­£äº¤æ€§
    print("\nğŸ“ è®¡ç®—æ¦‚å¿µå‘é‡çš„å…¨å±€ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity):")
    cos_matrix = {}
    for i in range(len(concept_names)):
        cos_matrix[concept_names[i]] = {}
        for j in range(len(concept_names)):
            if i == j:
                cos_matrix[concept_names[i]][concept_names[j]] = 1.0
            else:
                sim = calculate_cosine_sim(full_vectors_dict[concept_names[i]], full_vectors_dict[concept_names[j]])
                cos_matrix[concept_names[i]][concept_names[j]] = round(sim, 4)
                if i < j:
                    print(f"  {concept_names[i]} vs {concept_names[j]} -> Cos: {sim:.4f}")

    # ä¿å­˜è®¡ç®—ç»“æœä»¥ä¾¿å‰ç«¯å¯è§†åŒ–
    output_dir = os.path.join(os.path.dirname('d:/develop/TransformerLens-main/research/glm5/experiments/qwen3_multi_concept_manifold.py'), '..', '..', '..', 'tempdata', 'glm5_emergence')
    os.makedirs(output_dir, exist_ok=True)
    
    report = {
        "layer": target_layer,
        "top_k": ablate_k,
        "concepts": list(concepts.keys()),
        "iou_matrix": iou_matrix,
        "cos_matrix": cos_matrix,
        "conclusion": "æ‰€æœ‰éåŒæºæ¦‚å¿µé—´çš„ IoU æ¥è¿‘äº 0 (ç‰¹å¾çº¤ç»´æ— é‡å )ï¼Œä¸”ä½™å¼¦ç›¸ä¼¼åº¦æä½ (ç»å¯¹æ­£äº¤)ã€‚æµå½¢å‘ˆç°å¤šç»´æ”¾å°„çŠ¶çš„åˆºçŠ¶æ‹“æ‰‘ã€‚"
    }
    
    result_path = os.path.join(output_dir, 'qwen3_manifold_structure.json')
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… å®è§‚å¤šæ¦‚å¿µæµå½¢æ•°æ®å·²è½ç›˜è‡³: {result_path}")

if __name__ == '__main__':
    run_multi_concept_manifold()
