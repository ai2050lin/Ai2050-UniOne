# -*- coding: utf-8 -*-
"""
Qwen3 ç¼–ç ç»“æ„å››ç»´åº¦æå–å™¨
=========================
ä» Qwen3-4B ä¸­æå–ç¼–ç ï¼ŒéªŒè¯å››ä¸ªå…³é”®æ•°å­¦ç‰¹æ€§ï¼š
  1. é«˜ç»´æŠ½è±¡ â€” è¯­ä¹‰æ”¶æ•›èƒ½åŠ›
  2. ä½ç»´ç²¾ç¡® â€” ç»†ç²’åº¦åŒºåˆ†èƒ½åŠ›
  3. ç‰¹å¼‚æ€§ â€” æ¦‚å¿µå­ç©ºé—´æ­£äº¤æ€§
  4. ç³»ç»Ÿæ€§ â€” ç±»æ¯”å…³ç³»ä¸€è‡´æ€§

è¾“å‡º: tempdata/qwen3_structure_report.json + 4 å¼ å¯è§†åŒ–å›¾
"""

import json
import os
import sys
import time

import matplotlib

matplotlib.use("Agg")  # æ— å¤´æ¨¡å¼ï¼Œå…¼å®¹æœåŠ¡å™¨
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity

# ============================================================
# ç¬¬é›¶éƒ¨åˆ†ï¼šæ¨¡å‹åŠ è½½ï¼ˆå¤ç”¨å·²éªŒè¯çš„ import_trace.py é€»è¾‘ï¼‰
# ============================================================

SNAPSHOT_PATH = r"D:\develop\model\hub\models--Qwen--Qwen3-4B\snapshots\1cfa9a7208912126459214e8b04321603b3df60c"

# ç¯å¢ƒå˜é‡
os.environ["HF_HOME"] = r"D:\develop\model"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TORCH_FORCE_WEIGHTS_ONLY_LOAD"] = "0"


def load_qwen3():
    """åŠ è½½ Qwen3-4B ä¸º HookedTransformer"""
    import transformers.configuration_utils as config_utils
    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

    from transformer_lens import HookedTransformer

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[*] åŠ è½½ Qwen3-4Bï¼Œè®¾å¤‡: {device}")
    print(f"    è·¯å¾„: {SNAPSHOT_PATH}")

    t0 = time.time()

    # æ­¥éª¤ 1: åœ¨ CPU ä¸ŠåŠ è½½ HF æ¨¡å‹ (HookedTransformer ä¼šè‡ªè¡Œå¤„ç†è®¾å¤‡è¿ç§»)
    hf_model = AutoModelForCausalLM.from_pretrained(
        SNAPSHOT_PATH, local_files_only=True, trust_remote_code=True,
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(
        SNAPSHOT_PATH, local_files_only=True, add_bos_token=False
    )

    # ä¿®å¤1: Qwen3 tokenizer ç¼ºå°‘ bos_token
    if tokenizer.bos_token is None:
        tokenizer.bos_token = tokenizer.eos_token
        tokenizer.bos_token_id = tokenizer.eos_token_id
        print(f"    [fix] è®¾ç½® bos_token = eos_token ({tokenizer.bos_token})")

    # ä¿®å¤2: Monkey-patch PretrainedConfig ä»¥ä¿®å¤ rope_theta
    _orig_getattr = config_utils.PretrainedConfig.__getattribute__

    def _patched_getattr(self, key):
        if key == "rope_theta":
            try:
                return _orig_getattr(self, key)
            except AttributeError:
                try:
                    rs = _orig_getattr(self, "rope_scaling")
                    if isinstance(rs, dict) and "rope_theta" in rs:
                        return rs["rope_theta"]
                except (AttributeError, TypeError):
                    pass
                return 1000000
        return _orig_getattr(self, key)

    config_utils.PretrainedConfig.__getattribute__ = _patched_getattr

    # ä¿®å¤3: Monkey-patch get_tokenizer_with_bos é¿å…é‡æ–°åŠ è½½ tokenizer
    import transformer_lens.utils as tl_utils
    _orig_get_tok_bos = tl_utils.get_tokenizer_with_bos

    def _patched_get_tok_bos(tok):
        # ç›´æ¥è¿”å›å·²ä¿®å¤çš„ tokenizerï¼Œé¿å…é‡æ–° from_pretrained
        return tok

    tl_utils.get_tokenizer_with_bos = _patched_get_tok_bos
    print("    [fix] å·² monkey-patch rope_theta + get_tokenizer_with_bos")

    try:
        model = HookedTransformer.from_pretrained(
            "Qwen/Qwen3-4B", hf_model=hf_model, device=device, tokenizer=tokenizer,
            fold_ln=False, center_writing_weights=False, center_unembed=False,
            dtype=torch.float16, default_prepend_bos=False
        )
    finally:
        config_utils.PretrainedConfig.__getattribute__ = _orig_getattr
        tl_utils.get_tokenizer_with_bos = _orig_get_tok_bos
        print("    [fix] å·²æ¢å¤æ‰€æœ‰ monkey-patch")

    model.eval()
    print(f"[+] æ¨¡å‹åŠ è½½å®Œæˆ ({time.time() - t0:.1f}s)")
    print(f"    å±‚æ•°: {model.cfg.n_layers}, ç»´åº¦: {model.cfg.d_model}")
    return model


def run_qwen3_ablation():
    print("\nğŸ” å¯åŠ¨ Qwen3 çœŸå®ç‰¹å¾åŸå­æ¶ˆèåˆ‡ç‰‡æ‰‹æœ¯...")
    model = load_qwen3()
    
    prompt_A = "The capital of France is"
    target_A = " Paris"
    
    prompt_B = "The result of 2 + 3 is" 
    target_B = " 5"
    
    print(f"\n>> æµ‹å®šå¥åº·åŸºçº¿è¡¨ç°...")
    logits_A, cache_A = model.run_with_cache(prompt_A)
    logits_B = model(prompt_B)
    
    pred_token_A = logits_A[0, -1].argmax().item()
    pred_str_A = model.tokenizer.decode([pred_token_A])
    
    pred_token_B = logits_B[0, -1].argmax().item()
    pred_str_B = model.tokenizer.decode([pred_token_B])
    
    print(f"  [Task A å¥åº·] {prompt_A} -> '{pred_str_A}' (é¢„æœŸ: Paris)")
    print(f"  [Task B å¥åº·] {prompt_B} -> '{pred_str_B}' (é¢„æœŸ: 5)")
    
    target_layer = model.cfg.n_layers // 2 + 2 
    resid_post = cache_A[f"blocks.{target_layer}.hook_resid_post"][0, -1, :]
    
    ablate_k = 15
    import torch
    import json
    top_indices = torch.topk(resid_post.abs(), ablate_k).indices.tolist()
    print(f"\nğŸ§  ç‰©ç†å®šä½æ¢æµ‹å®Œæ¯•:")
    print(f"   é”å®šåœ¨ç¬¬ {target_layer} æ®‹å·®å±‚ï¼Œè¯†åˆ«åˆ°ä¸“å¸å¤„ç†å½“å‰ä¸Šä¸‹æ–‡çš„ {ablate_k} æ ¹æœ€å¼ºç‰¹å¾çº¤ç»´ã€‚")
    print(f"   å‡†å¤‡é’ˆå¯¹å…¶æ‰§è¡Œè„‘æŸä¼¤æ‰‹æœ¯ï¼Œé˜»æ–­è¿™äº›ç»´åº¦ï¼š{top_indices}")
    
    def ablation_hook(resid, hook):
        resid[:, -1, top_indices] = 0.0
        return resid
        
    print(f"\nğŸ©¸ æ­£åœ¨æ‰§è¡Œå®šç‚¹è„‘åˆ‡é™¤æ‰‹æœ¯...")
    ablation_logits_A = model.run_with_hooks(
        prompt_A,
        fwd_hooks=[(f"blocks.{target_layer}.hook_resid_post", ablation_hook)]
    )
    
    ablation_logits_B = model.run_with_hooks(
        prompt_B,
        fwd_hooks=[(f"blocks.{target_layer}.hook_resid_post", ablation_hook)]
    )
    
    abl_pred_str_A = model.tokenizer.decode([ablation_logits_A[0, -1].argmax().item()])
    abl_pred_str_B = model.tokenizer.decode([ablation_logits_B[0, -1].argmax().item()])
    
    print(f"\n>> åˆ‡ç‰‡æ¶ˆèåç»“æœæ ¸æŸ¥:")
    print(f"  [Task A é˜»æ–­å] {prompt_A} -> '{abl_pred_str_A}'")
    print(f"  [Task B æ—è·¯å] {prompt_B} -> '{abl_pred_str_B}'")
    
    if abl_pred_str_A.strip().lower() != target_A.strip().lower() and abl_pred_str_B.strip() == target_B.strip():
        conclusion = "å®Œç¾å¤ç°ï¼æˆ‘ä»¬åœ¨æ‹¥æœ‰æ•°åäº¿å‚æ•°çš„çœŸå®å¤§æ¨¡å‹èº«ä¸Šç²¾å‡†å‰”é™¤æ‰äº†é‚£åå‡ æ ¹ä¸“å¸ç‰¹å®šçŸ¥è¯†æå–çš„ç¥ç»çº¤ç»´ï¼Œå¯¼è‡´äº†ç›®æ ‡çŸ¥è¯†çš„æå–å®Œå…¨å´©æºƒï¼Œè€Œæ—è·¯é€»è¾‘çŸ¥è¯†ï¼ˆç®—æœ¯ï¼‰å®Œå¥½æ— æŸã€‚è¿™ç›´æ¥è¯æ˜äº†çœŸå® LLM ä¸­åŒæ ·å­˜åœ¨æåº¦æ­£äº¤è§£è€¦çš„é«˜ç»´ç¨€ç–å‡ ä½•åŸå­ã€‚"
    else:
        conclusion = "å‡ºç°æ³›åŒ–çº§è”å½±å“æˆ–å—é˜»ä¸æ˜æ˜¾ã€‚åœ¨æå…¶åºå¤§çš„æ¨¡å‹ä¸­ï¼Œå¯èƒ½ç‰¹å¾æ•£å¸ƒåœ¨å¤šå±‚ï¼Œæˆ–è€…åˆ‡é™¤çš„çº¤ç»´ä¹Ÿæ³¢åŠäº†å…¶ä»–æ—è·¯ã€‚"
        
    print(f"\nğŸ§  Qwen3 çœŸæœºå®éªŒç»“è®º: {conclusion}")
    
    output_dir = os.path.join(os.path.dirname('d:/develop/TransformerLens-main/research/glm5/experiments/qwen3_feature_ablation.py'), '..', '..', '..', 'tempdata', 'glm5_emergence')
    os.makedirs(output_dir, exist_ok=True)
    result = {
        "experiment": "Qwen3 Feature Atom Ablation",
        "model": "Qwen/Qwen3-4B",
        "layer_ablated": int(target_layer),
        "indices_ablated": top_indices,
        "health_status": {"task_A": pred_str_A, "task_B": pred_str_B},
        "ablated_status": {"task_A": abl_pred_str_A, "task_B": abl_pred_str_B},
        "conclusion": conclusion
    }
    
    result_path = os.path.join(output_dir, 'qwen3_feature_ablation.json')
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"âœ… Qwen3 ç‰©ç†å¹²é¢„åˆ‡ç‰‡æ•°æ®å·²ä¿å­˜è‡³: {result_path}")

if __name__ == '__main__':
    run_qwen3_ablation()
