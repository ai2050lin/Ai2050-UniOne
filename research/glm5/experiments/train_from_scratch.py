# -*- coding: utf-8 -*-
"""
GLM5è·¯çº¿ - Phase 1: ç‰¹å¾æ¶Œç°è¿½è¸ª
æ­¤å®éªŒä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒä¸€ä¸ªå°å‹çš„ Transformer æ¨¡å‹ï¼ˆæˆ–MLPå±‚ï¼‰ï¼Œ
æ¯100æ­¥è®°å½•æ¿€æ´»çŠ¶æ€ï¼Œè¿½è¸ªæœ‰æ•ˆç§©å’Œç¨€ç–åº¦çš„å˜åŒ–ï¼Œä»¥æ­ç¤ºç‰¹å¾æ˜¯å¦‚ä½•ä»æ— åˆ°æœ‰ã€ä»åˆ†åŒ–åˆ°ç»„åˆçš„ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import json
import os

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
output_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'tempdata', 'glm5_emergence')
os.makedirs(output_dir, exist_ok=True)

class SimpleTransformerLayer(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        # ç®€åŒ–çš„å±‚ç»“æ„ï¼šçº¿æ€§å±‚æ¨¡æ‹Ÿç‰¹å¾æå–
        self.fc1 = nn.Linear(d_model, d_model * 4)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_model * 4, d_model)
        
    def forward(self, x):
        h = self.act(self.fc1(x))
        out = self.fc2(h)
        return out + x, h  # è¿”å›è¾“å‡ºå’Œéšè—å±‚æ¿€æ´»ä»¥ä¾›è¿½è¸ª

class SimpleTransformer(nn.Module):
    def __init__(self, num_layers=4, d_model=128):
        super().__init__()
        self.embedding = nn.Linear(d_model, d_model)
        self.layers = nn.ModuleList([SimpleTransformerLayer(d_model) for _ in range(num_layers)])
        self.head = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        x = self.embedding(x)
        activations = []
        for layer in self.layers:
            x, act = layer(x)
            activations.append(act)
        out = self.head(x)
        return out, activations

def calculate_effective_rank(act):
    """è®¡ç®—ç‰¹å¾çŸ©é˜µçš„æœ‰æ•ˆç§© (åŸºäºå¥‡å¼‚å€¼åˆ†è§£ç†µ)"""
    # å‹å¹³ batch å’Œ sequence ç»´åº¦
    flat_act = act.view(-1, act.size(-1))
    if flat_act.size(0) < flat_act.size(1):
        # ç¡®ä¿å¥‡å¼‚å€¼åˆ†è§£èƒ½å¤Ÿè¿è¡Œ
        return 0.0
    
    # éšæœºé‡‡æ ·ä»¥åŠ å¿«è®¡ç®—
    if flat_act.size(0) > 1000:
        indices = torch.randperm(flat_act.size(0))[:1000]
        flat_act = flat_act[indices]
        
    flat_act = flat_act - flat_act.mean(dim=0)
    try:
        _, S, _ = torch.svd(flat_act)
        # è®¡ç®—å½’ä¸€åŒ–çš„å¥‡å¼‚å€¼é¢‘ç‡
        P = S / S.sum()
        # è®¡ç®—é¦™å†œç†µ
        entropy = -torch.sum(P * torch.log(P + 1e-9))
        # æœ‰æ•ˆç§© = exp(entropy)
        effective_rank = torch.exp(entropy).item()
        return effective_rank
    except Exception:
        return 0.0

def calculate_sparsity(act):
    """è®¡ç®—ç¨€ç–åº¦ (L0çš„è¿‘ä¼¼)"""
    # è¿™é‡Œç®€å•ä½¿ç”¨éé›¶å…ƒç´ çš„æ¯”ä¾‹ï¼Œå¯¹äºGELUæ¥è¯´ï¼Œå°±æ˜¯å°äºå¾ˆå°æ•°çš„ä¸ºé›¶
    threshold = 1e-3
    zeros = (act.abs() < threshold).float().mean().item()
    return zeros * 100  # è½¬ä¸ºç™¾åˆ†æ¯”

def run_experiment():
    print("ğŸš€ å¯åŠ¨ GLM5 ç‰¹å¾æ¶Œç°è¿½è¸ªå®éªŒ...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    d_model = 128
    num_layers = 4
    model = SimpleTransformer(num_layers=num_layers, d_model=d_model).to(device)
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒå¾ªç¯
    total_steps = 3000
    tracking_interval = 100
    
    batch_size = 64
    seq_len = 32
    
    records = []
    
    start_time = time.time()
    
    print("\nå¼€å§‹è®­ç»ƒä¸ç‰¹å¾è¿½è¸ª...")
    print("Step | Loss | L0 Sparsity (Layer Avg) | Effective Rank (Layer 0->1->2->3)")
    print("-" * 80)
    
    for step in range(total_steps + 1):
        # ä½¿ç”¨éšæœºå™ªå£°æ¨¡æ‹Ÿä¿¡å·æµ
        # åœ¨çœŸå®ä»»åŠ¡ä¸­è¿™ä¼šè¢«æ›¿æ¢ä¸ºç¡®åˆ‡çš„è¾“å…¥æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ token æˆ–å›¾åƒç‰¹å¾ï¼‰
        x = torch.randn(batch_size, seq_len, d_model).to(device)
        target = torch.roll(x, shifts=-1, dims=1)  # æ¨¡æ‹Ÿé¢„æµ‹ä¸‹ä¸€æ­¥çš„è‡ªå›å½’ä»»åŠ¡
        
        optimizer.zero_grad()
        out, activations = model(x)
        loss = criterion(out, target)
        
        loss.backward()
        optimizer.step()
        
        # å®šæœŸè¿½è¸ª
        if step % tracking_interval == 0:
            layer_ranks = []
            layer_sparsities = []
            
            with torch.no_grad():
                for act in activations:
                    rank = calculate_effective_rank(act)
                    sparsity = calculate_sparsity(act)
                    
                    layer_ranks.append(f"{rank:.1f}")
                    layer_sparsities.append(sparsity)
            
            avg_sparsity = sum(layer_sparsities) / len(layer_sparsities)
            
            print(f"{step:4d} | {loss.item():.4f} | {avg_sparsity:5.1f}% | {' -> '.join(layer_ranks)}")
            
            records.append({
                "step": step,
                "loss": loss.item(),
                "sparsities": layer_sparsities,
                "effective_ranks": [float(r) for r in layer_ranks]
            })

    end_time = time.time()
    runtime = end_time - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {runtime:.2f}ç§’")
    
    # ä¿å­˜ç»“æœ
    result_path = os.path.join(output_dir, 'emergence_tracking.json')
    result = {
        "metadata": {
            "model": "SimpleTransformer",
            "layers": num_layers,
            "d_model": d_model,
            "parameters": total_params,
            "total_steps": total_steps,
            "runtime_seconds": runtime,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        },
        "records": records
    }
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
        
    print(f"æ•°æ®å·²ä¿å­˜è‡³: {result_path}")
    print("è¿™ä¸€æ•°æ®ä¸ºå‰ç«¯ GLM5Tab.jsx ä¸­ test-000b æµ‹è¯•è®°å½•æä¾›äº†æ•°å­¦ä¸ç‰©ç†åŸºç¡€ã€‚")

if __name__ == "__main__":
    run_experiment()
