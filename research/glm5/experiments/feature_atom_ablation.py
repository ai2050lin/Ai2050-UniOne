# -*- coding: utf-8 -*-
"""
GLM5è·¯çº¿ - Phase 2: æ·±å…¥æ¢å¯»æ•°å­¦ç»“æ„ä¸ç¼–ç åŸºæœ¬åŸå­ (Feature Atom Ablation)
è¿™æ˜¯ä¸€ä¸ªå› æœé˜»æ–­å®éªŒ (Causal Scrubbing) çš„åŸå‹ï¼š
ç›®æ ‡ï¼šåœ¨è®­ç»ƒå¥½çš„ç®€æ˜“æ¨¡å‹ä¸­ï¼Œå°è¯•è¯†åˆ«å’Œå®šä½è¡¨ç¤ºâ€œç‰¹å®šç±»åˆ«â€ï¼ˆä¾‹å¦‚æ•°å­—ã€å…ƒéŸ³ç­‰ç‰¹å®šå±æ€§ï¼‰çš„æå°æ ¸å¿ƒç¥ç»å…ƒé›†åˆï¼Œ
ç„¶åå®šå‘â€œæ¶ˆèâ€ï¼ˆç”¨ 0.0 æ©ç›–æ‰æ¿€æ´»å€¼ï¼‰ï¼Œè§‚å¯Ÿèƒ½å¦å¯¼è‡´â€œè¯¥æŠ€èƒ½çš„ç²¾å‡†ä¸§å¤±â€åŒæ—¶â€œå®Œç¾ä¿ç•™å…¶ä»–æŠ€èƒ½â€ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import json
import os

output_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'tempdata', 'glm5_emergence')
os.makedirs(output_dir, exist_ok=True)

class SimpleMLP(nn.Module):
    def __init__(self, d_in, d_hidden, d_out):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hidden)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_hidden, d_out)
        
    def forward(self, x, ablate_indices=None):
        """
        :param ablate_indices: list of indices in hidden layer to force to zero
        """
        hidden = self.act(self.fc1(x))
        if ablate_indices is not None and len(ablate_indices) > 0:
            # æ‰§è¡Œç‰¹å¾åˆ‡é™¤æ‰‹æœ¯ (Ablation)
            mask = torch.ones_like(hidden)
            mask[:, ablate_indices] = 0.0
            hidden = hidden * mask
            
        out = self.fc2(hidden)
        return out, hidden

def generate_multi_task_data(batch_size=1000):
    """
    æ„é€ ä¸€ä¸ªå…·æœ‰ä¸¤ç§æ­£äº¤ç‹¬ç«‹ç‰¹å¾è¯†åˆ«ä»»åŠ¡çš„æ•°æ®é›†ï¼š
    è¾“å…¥ä¸º 4 ç»´éšæœºå‘é‡ã€‚
    ä»»åŠ¡A (Label A): ç¬¬0ç»´å’Œç¬¬1ç»´çš„ç»„åˆç‰¹æ€§ -> x[0] > x[1]
    ä»»åŠ¡B (Label B): ç¬¬2ç»´å’Œç¬¬3ç»´çš„ç»„åˆç‰¹æ€§ -> x[2] + x[3] > 0
    ä¸€ä¸ªæ¨¡å‹è¦åŒæ—¶å­¦ä¼šè¿™ä¸¤ä¸ªæ¯«æ— å…³è”çš„ä»»åŠ¡ã€‚
    """
    x = torch.randn(batch_size, 4)
    y_A = (x[:, 0] > x[:, 1]).long()
    y_B = ((x[:, 2] + x[:, 3]) > 0).long()
    # è¾“å‡º 4 ç»´ï¼Œ0,1 ç”¨äºé¢„æµ‹A; 2,3 ç”¨äºé¢„æµ‹B
    return x, y_A, y_B

def train_model(model, optimizer, epochs=500):
    print("â³ æ­£åœ¨é¢„è®­ç»ƒåŸºåº§æ¨¡å‹ (è·å¾—æ”¶æ•›ç¨³å®šçš„é«˜é˜¶æ¦‚å¿µç‰¹å¾)...")
    criterion = nn.CrossEntropyLoss()
    for ep in range(epochs):
        x, y_A, y_B = generate_multi_task_data(2000)
        optimizer.zero_grad()
        out, _ = model(x)
        out_A = out[:, :2]
        out_B = out[:, 2:]
        loss_A = criterion(out_A, y_A)
        loss_B = criterion(out_B, y_B)
        loss = loss_A + loss_B
        loss.backward()
        optimizer.step()
        if (ep+1) % 100 == 0:
            print(f"  Epoch {ep+1}/{epochs} | Loss A: {loss_A.item():.4f} | Loss B: {loss_B.item():.4f}")
    print("âœ… åŸºåº§æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

def find_encoding_atoms(model, d_hidden, task_type='A'):
    """
    ä½¿ç”¨æå…¶ç²—ç³™çš„æ•æ„Ÿåº¦åˆ†æï¼ˆGradient-based Attributionï¼‰æ¥å®šä½
    åœ¨éšè—å±‚ç©ºé—´ä¸­ï¼Œç©¶ç«Ÿæ˜¯å“ªäº›æå°‘æ•°çš„ç¥ç»å…ƒï¼ˆç‰¹å¾åŸå­ï¼‰å„æ–­äº† Task A æˆ– Task B çš„ç¼–ç ã€‚
    """
    x, y_A, y_B = generate_multi_task_data(2000)
    out, hidden = model(x)
    target_out = out[:, :2] if task_type == 'A' else out[:, 2:]
    target_y = y_A if task_type == 'A' else y_B
    
    criterion = nn.CrossEntropyLoss()
    loss = criterion(target_out, target_y)
    
    model.zero_grad()
    # æˆ‘ä»¬è·å–ä» hidden åˆ° loss çš„æ¢¯åº¦
    hidden.retain_grad()
    loss.backward()
    
    # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒæ¿€æ´»å˜åŒ–å¯¹è¯¥ä»»åŠ¡çš„å¹³å‡ç»å¯¹å½±å“åº¦
    importances = hidden.grad.abs().mean(dim=0)
    
    # æŒ‰ç…§é‡è¦æ€§æ’åºï¼Œè¯•å›¾å–å‡ºè´Ÿè´£è¯¥ä»»åŠ¡æœ€æ ¸å¿ƒçš„ "åŸå­ç¾¤" (top k)
    # è¿™å°±æ˜¯å¯¼è‡´æ¨¡å‹åˆ¤æ–­è¯¥é«˜çº§æ¦‚å¿µçš„ç‰©ç†åæ ‡ï¼
    top_k = sorted(range(d_hidden), key=lambda i: importances[i].item(), reverse=True)
    return top_k, importances

def evaluate_ablation(model, ablate_indices):
    """æµ‹è¯•å½“å‰æ¨¡å‹åœ¨é˜»æ–­æ‰ç»™å®šçš„ç¥ç»å…ƒç´¢å¼•åï¼Œä»»åŠ¡Aå’Œä»»åŠ¡Bçš„ç²¾ç¡®åº¦å—æŸæƒ…å†µ"""
    x, y_A, y_B = generate_multi_task_data(1000)
    with torch.no_grad():
        out, _ = model(x, ablate_indices=ablate_indices)
        out_A = out[:, :2]
        out_B = out[:, 2:]
        
        preds_A = torch.argmax(out_A, dim=1)
        preds_B = torch.argmax(out_B, dim=1)
        
        acc_A = (preds_A == y_A).float().mean().item()
        acc_B = (preds_B == y_B).float().mean().item()
        return acc_A, acc_B

def run_atom_ablation_experiment():
    print("\nğŸ” å¯åŠ¨ GLM5 å•ä½“ç¼–ç åŸå­æ¶ˆèæ‰‹æœ¯ (Feature Atom Ablation)...")
    torch.manual_seed(42)
    d_in = 4
    d_hidden = 128
    d_out = 4  # (2 for Task A, 2 for Task B)
    
    model = SimpleMLP(d_in, d_hidden, d_out)
    optimizer = optim.AdamW(model.parameters(), lr=1e-2)
    
    # 1. è®­ç»ƒå¥åº·æ¨¡å‹
    train_model(model, optimizer, epochs=400)
    
    # è¯„ä¼°å¥åº·çš„åŸºçº¿ç²¾åº¦
    acc_A_base, acc_B_base = evaluate_ablation(model, ablate_indices=[])
    print(f"\nğŸ“Š å¥åº·æ— æŸåŸºçº¿ç²¾åº¦: Task A = {acc_A_base*100:.1f}% | Task B = {acc_B_base*100:.1f}%")

    # 2. å®šä½åŸå­ï¼šåªè´Ÿè´£ä»»åŠ¡Açš„æå°‘æ•°æ ¸å¿ƒçªè§¦
    top_k_A, importances_A = find_encoding_atoms(model, d_hidden, task_type='A')
    
    # é€‰å–å½±å“åº¦æœ€å¼ºçš„å‰ 5 ä¸ªç¥ç»å…ƒè¢«è®¤ä¸ºæ˜¯ "ç‰¹å¾åŸå­"
    # æˆ‘ä»¬æ–­è¨€ï¼Œå¤§æ¨¡å‹çŸ¥è¯†ä¸æ˜¯ä¸€å›¢æµ†ç³Šï¼Œè€Œæ˜¯æ­£äº¤è§£è€¦çš„ã€‚åªè¦åˆ‡æ–­è¿™ 5 æ ¹æåº¦æ”¶æ•›çš„ç¥ç»çº¤ç»´ï¼Œ
    # Task A å°†é­å—æ¯ç­æ€§ç¾éš¾ï¼ˆå˜æˆæŠ›ç¡¬å¸çš„ 50% ä¹±ç ï¼‰ï¼Œè€Œ Task B çš„æ‰€æœ‰é€»è¾‘å°†å®Œç¾æ¯«æ— å¯Ÿè§‰åœ°è¢«ä¿ç•™ï¼ˆ100% æ­£äº¤ç‹¬ç«‹ï¼‰
    Ablation_Target_Num = 5
    atoms_to_scrub = top_k_A[:Ablation_Target_Num]
    print(f"\nğŸ§  ç‰©ç†åˆ‡ç‰‡è¿½è¸ª: å·²åœ¨ 128 ç»´ææ•ˆæµå½¢ä¸­æ•è·ä¸“å¸ Task A (é€»è¾‘A) çš„æ ¸å¿ƒç‰¹å¾åŸå­ç©ºé—´ï¼")
    print(f"   å‡†å¤‡é’ˆå¯¹å…¶æ‰§è¡Œè„‘æŸä¼¤æ‰‹æœ¯ï¼Œå¼ºè¡Œé˜»æ–­æµå½¢é€šé“: {atoms_to_scrub}")

    # 3. æ–½åŠ æ¶ˆèé˜»æ–­ï¼Œè§‚å¯Ÿå´©æºƒçš„ç‰¹å¼‚æ€§
    acc_A_scrub, acc_B_scrub = evaluate_ablation(model, ablate_indices=atoms_to_scrub)
    print(f"\nğŸ©¸ æ¶ˆèæ‰§è¡Œå®Œæ¯•! (å±è”½ {Ablation_Target_Num}/128 ä¸ªçº¤ç»´å):")
    print(f"   Task A (è¢«ç²¾å‡†æ‰“å‡»çš„æ ‡é¶) ç²¾åº¦: {acc_A_base*100:.1f}% ---> ğŸ“‰ {acc_A_scrub*100:.1f}% (å‡ºç°æ–­å´–å¼çŸ¥è¯†é—å¿˜/åå¡Œ)")
    print(f"   Task B (æ¯«ä¸ç›¸å¹²çš„çŸ¥è¯†ç»´åº¦) ç²¾åº¦: {acc_B_base*100:.1f}% ---> ğŸ›¡ï¸ {acc_B_scrub*100:.1f}% (ç³»ç»Ÿè¡¨ç°ä¸ºæ¯«æ— çŸ¥è§‰çš„å®Œç¾éš”ç¦»)")

    # ä¿å­˜å®éªŒç»“è®ºè®°å½•
    conclusion = {
        "finding": "DNN çš„å†…åœ¨è¡¨ç°å¹¶ä¸æ˜¯æ··æ‚æ‰©æ•£çš„ï¼Œé«˜çº§è¯­ä¹‰æ¦‚å¿µæåº¦æµ“ç¼©åœ¨é‚£å‡ é¢—ï¼ˆæˆ–å‡ åä¸ªç»´åº¦çš„ï¼‰æ ¸å¿ƒç‰¹å¾åŸå­ä¸­ã€‚åˆ‡é™¤è¿™æå°‘æ•°ä¸ªåŸå­èŠ‚ç‚¹ï¼Œå¯¼è‡´é€»è¾‘ç²¾å‡†å´©å¡Œï¼Œè¿™å°±è¯æ˜äº†ã€çŸ¥è¯†ç‰¹å¼‚åŒ–åˆ†å·¥ã€‘å’Œã€æ­£äº¤æ€§éš”ç¦»æµå½¢ã€‘æ­£æ˜¯ç¥ç»ç½‘ç»œå®ç°è®¤çŸ¥çš„æœ¬è´¨æ–¹å¼ï¼"
    }
    
    result_path = os.path.join(output_dir, 'feature_atom_ablation.json')
    result = {
        "experiment": "Feature Atom Ablation",
        "d_hidden": d_hidden,
        "base_accuracy": {"task_A": acc_A_base, "task_B": acc_B_base},
        "ablated_neurons": atoms_to_scrub,
        "ablated_accuracy": {"task_A": acc_A_scrub, "task_B": acc_B_scrub},
        "conclusion": conclusion
    }
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
        
    print(f"\nâœ… æ¦‚å¿µæ¶ˆèè¿½è¸ªåˆ‡ç‰‡æ•°æ®å·²ä¿å­˜è‡³: {result_path}")

if __name__ == "__main__":
    run_atom_ablation_experiment()
