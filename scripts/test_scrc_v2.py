# -*- coding: utf-8 -*-
"""
================================================================================
SCRC éªŒè¯å®éªŒ V2ï¼šä¿®æ­£ç‰ˆç¨€ç–ç«äº‰å¾ªç¯å›è·¯
================================================================================
V1 é—®é¢˜è¯Šæ–­ï¼š
  1. çº¯ Hebbian å¤–ç§¯å¯¼è‡´æ‰€æœ‰æ¨¡æ¿åç¼©åˆ°æ•°æ®å‡å€¼æ–¹å‘ï¼ˆç¼ºå°‘ç«äº‰æ€§åˆ†åŒ–ï¼‰
  2. æ²¡æœ‰è¾“å…¥å½’ä¸€åŒ–ï¼Œå†…ç§¯å°ºåº¦ä¸å¯æ§
  3. ç¼ºå°‘"å Hebbian"æœºåˆ¶â€”â€”æœªè¢«é€‰ä¸­çš„æ¨¡æ¿ä¸ä¼šè¿œç¦»è¾“å…¥

V2 ä¿®æ­£ï¼š
  1. ç«äº‰æ€§ Hebbianï¼ˆOja's Ruleï¼‰ï¼šè‡ªåŠ¨å½’ä¸€åŒ–ï¼Œé˜²æ­¢æ¨¡æ¿åç¼©
  2. Winner-Take-All + è´Ÿåé¦ˆï¼šèµ¢å®¶é è¿‘è¾“å…¥ï¼Œè¾“å®¶è¿œç¦»
  3. è¾“å…¥ L2 å½’ä¸€åŒ–
  
æ ¸å¿ƒå…¬å¼ä¸å˜ï¼šZ = top_k(W Â· X)
ä½†å­¦ä¹ è§„åˆ™å‡çº§ä¸ºç«äº‰æ€§ Oja è§„åˆ™ï¼š
  å¯¹èµ¢å®¶: Î”w_j = Î· Â· (x - (w_jÂ·x)Â·w_j)    [é è¿‘è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒå•ä½èŒƒæ•°]
  å¯¹è¾“å®¶: ä¸æ›´æ–°ï¼ˆç”Ÿç‰©å­¦ä¸­çš„æ²‰é»˜ç¥ç»å…ƒä¸å¯å¡‘ï¼‰
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time
import os
import json

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"[è®¾å¤‡] ä½¿ç”¨: {device}")
if device.type == 'cuda':
    print(f"[GPU] {torch.cuda.get_device_name(0)}")

# æ•°æ®
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST('./tempdata', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./tempdata', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)


class SCRC_v2(nn.Module):
    """
    ç¨€ç–ç«äº‰å¾ªç¯å›è·¯ V2
    
    ä¿®æ­£ç‰ˆä½¿ç”¨ç«äº‰æ€§ Hebbian å­¦ä¹ ï¼ˆOja's Rule å˜ä½“ï¼‰ï¼š
    - èµ¢å®¶ï¼ˆtop-k æ¿€æ´»çš„å•å…ƒï¼‰ï¼šæ¨¡æ¿å‘è¾“å…¥æ–¹å‘ç§»åŠ¨
    - è¾“å®¶ï¼šä¿æŒä¸å˜ï¼ˆç”Ÿç‰©å­¦ä¸­æ²‰é»˜ç¥ç»å…ƒä¸æ”¹å˜çªè§¦ï¼‰
    - æ‰€æœ‰æ¨¡æ¿å§‹ç»ˆä¿æŒå•ä½èŒƒæ•°ï¼ˆè‡ªåŠ¨å½’ä¸€åŒ–ï¼‰
    
    è¿™æ›´æ¥è¿‘å¤§è„‘çš®å±‚çš„çœŸå®å­¦ä¹ æœºåˆ¶ã€‚
    """
    def __init__(self, input_dim, num_units, k, lr=0.01):
        super().__init__()
        self.k = k
        self.lr = lr
        self.num_units = num_units
        self.input_dim = input_dim
        
        # åˆå§‹åŒ–ä¸ºå•ä½èŒƒæ•°éšæœºæ–¹å‘
        W = torch.randn(num_units, input_dim, device=device)
        W = F.normalize(W, dim=1)
        self.W = nn.Parameter(W, requires_grad=False)
        
    def forward(self, x):
        # L2 å½’ä¸€åŒ–è¾“å…¥
        x_norm = F.normalize(x, dim=1)
        
        # å…´å¥‹æŠ•å°„ï¼šä½™å¼¦ç›¸ä¼¼åº¦
        scores = x_norm @ self.W.T  # [batch, num_units]
        
        # ç«äº‰æŠ‘åˆ¶ï¼šTop-K
        topk_vals, topk_idx = torch.topk(scores, self.k, dim=1)
        
        # ç¨€ç–è¾“å‡º
        z = torch.zeros_like(scores)
        z.scatter_(1, topk_idx, topk_vals.clamp(min=0))
        
        return z, x_norm, topk_idx
    
    def learn(self, x_norm, topk_idx):
        """
        ç«äº‰æ€§ Hebbian å­¦ä¹ ï¼ˆOja's Rule æ‰¹é‡ç‰ˆï¼‰
        
        å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåªæœ‰ top-k èµ¢å®¶çš„æ¨¡æ¿ä¼šæ›´æ–°ï¼š
        Î”w_j = Î· * (x - (w_jÂ·x) * w_j)
        
        è¿™ä¿è¯äº†ï¼š
        1. æ¨¡æ¿å‘è¾“å…¥æ–¹å‘ç§»åŠ¨ï¼ˆå­¦ä¹ æ–°ç‰¹å¾ï¼‰
        2. ||w_j|| å§‹ç»ˆä¿æŒä¸º 1ï¼ˆOja å½’ä¸€åŒ–ï¼‰
        3. åªæœ‰èµ¢å®¶æ›´æ–°ï¼ˆç«äº‰åˆ†åŒ–ï¼‰
        """
        batch_size = x_norm.shape[0]
        
        # å±•å¹³ topk_idx
        for b in range(min(batch_size, 64)):  # é™åˆ¶æ‰¹é‡æ›´æ–°é¿å…è¿‡æ…¢
            x_b = x_norm[b]  # [D]
            winners = topk_idx[b]  # [k]
            
            for j in winners:
                w_j = self.W.data[j]  # [D]
                proj = (w_j @ x_b)  # æ ‡é‡
                # Oja's rule
                delta = self.lr * (x_b - proj * w_j)
                self.W.data[j] += delta
        
        # é‡æ–°å½’ä¸€åŒ–
        self.W.data = F.normalize(self.W.data, dim=1)


class SCRC_Classifier_v2(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸¤çº§ SCRC
        self.scrc1 = SCRC_v2(784, 500, k=25, lr=0.05)
        self.scrc2 = SCRC_v2(500, 200, k=10, lr=0.05)
        # çº¿æ€§è¯»å‡º
        self.readout = nn.Linear(200, 10).to(device)
        
    def forward(self, x, learn=False):
        z1, x1_norm, idx1 = self.scrc1(x)
        z2, z1_norm, idx2 = self.scrc2(z1)
        
        if learn:
            self.scrc1.learn(x1_norm, idx1)
            self.scrc2.learn(z1_norm, idx2)
        
        logits = self.readout(z2)
        return logits, z1, z2


class SimpleAttention(nn.Module):
    """Attention å¯¹ç…§ç»„ï¼ˆä¸ V1 ç›¸åŒï¼‰"""
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.embed = nn.Linear(input_dim, hidden_dim)
        self.W_Q = nn.Linear(hidden_dim, hidden_dim)
        self.W_K = nn.Linear(hidden_dim, hidden_dim)
        self.W_V = nn.Linear(hidden_dim, hidden_dim)
        self.head = nn.Linear(hidden_dim, num_classes)
        self.hidden_dim = hidden_dim
        
    def forward(self, x):
        h = F.relu(self.embed(x))
        Q, K, V = self.W_Q(h), self.W_K(h), self.W_V(h)
        attn = F.softmax(Q * K / (self.hidden_dim ** 0.5), dim=-1)
        return self.head(attn * V)


class SimpleMLP(nn.Module):
    """æœ´ç´  MLP å¯¹ç…§ç»„â€”â€”æœ€åŸºç¡€çš„åŸºçº¿"""
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 500)
        self.fc2 = nn.Linear(500, 200)
        self.fc3 = nn.Linear(200, 10)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


def train_bp_model(model, name, epochs=5):
    """ç”¨åå‘ä¼ æ’­è®­ç»ƒä¸€ä¸ªæ¨¡å‹"""
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    t0 = time.time()
    for epoch in range(epochs):
        correct = total = 0
        loss_sum = 0
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            logits = model(data)
            loss = criterion(logits, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
            loss_sum += loss.item()
        print(f"  [{name}] Epoch {epoch+1}/{epochs} | æŸå¤±: {loss_sum/len(train_loader):.4f} | å‡†ç¡®ç‡: {correct/total*100:.2f}%")
    
    train_time = time.time() - t0
    
    # æµ‹è¯•
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits = model(data)
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
    test_acc = correct / total * 100
    params = sum(p.numel() for p in model.parameters())
    return test_acc, train_time, params


def train_scrc(epochs=5):
    """è®­ç»ƒ SCRCï¼ˆHebbian + è¯»å‡ºå¤´ BP æ··åˆï¼‰"""
    model = SCRC_Classifier_v2().to(device)
    optimizer = torch.optim.Adam(model.readout.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    params = sum(p.numel() for p in model.parameters())
    
    t0 = time.time()
    for epoch in range(epochs):
        correct = total = 0
        loss_sum = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            logits, _, _ = model(data, learn=True)
            loss = criterion(logits, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
            loss_sum += loss.item()
            
            if (batch_idx+1) % 50 == 0:
                print(f"  [SCRC] Epoch {epoch+1} | æ‰¹æ¬¡ {batch_idx+1}/{len(train_loader)} | å‡†ç¡®ç‡: {correct/total*100:.2f}%")
        
        print(f"  [SCRC] Epoch {epoch+1}/{epochs} | æŸå¤±: {loss_sum/len(train_loader):.4f} | å‡†ç¡®ç‡: {correct/total*100:.2f}%")
    
    train_time = time.time() - t0
    
    # æµ‹è¯•
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits, z1, z2 = model(data, learn=False)
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
    test_acc = correct / total * 100
    
    # ç¨€ç–æ€§åˆ†æ
    with torch.no_grad():
        sample_data = next(iter(test_loader))[0].to(device)
        _, z1, z2 = model(sample_data, learn=False)
        sp1 = (z1 == 0).float().mean().item() * 100
        sp2 = (z2 == 0).float().mean().item() * 100
        print(f"\n  [SCRC ç¨€ç–æ€§] ç¬¬1å±‚: {sp1:.1f}% é›¶æ¿€æ´» | ç¬¬2å±‚: {sp2:.1f}% é›¶æ¿€æ´»")
    
    # æ¨¡æ¿å¤šæ ·æ€§åˆ†æ
    W1 = model.scrc1.W.data
    W1_norm = F.normalize(W1, dim=1)
    sim = (W1_norm @ W1_norm.T)
    mask = ~torch.eye(sim.size(0), dtype=torch.bool, device=device)
    avg_sim = sim[mask].mean().item()
    print(f"  [SCRC æ¨¡æ¿å¤šæ ·æ€§] ç¬¬1å±‚æ¨¡æ¿å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {avg_sim:.4f}")
    
    return test_acc, train_time, params, model


if __name__ == '__main__':
    print("="*70)
    print("  SCRC V2 éªŒè¯å®éªŒï¼šç«äº‰æ€§ Hebbian ç¨€ç–å›è·¯")
    print("="*70)
    
    # 1. SCRC V2
    print("\n" + "-"*60)
    print("  [A] SCRC V2ï¼ˆç«äº‰æ€§ Hebbian + çº¿æ€§è¯»å‡º BPï¼‰")
    print("-"*60)
    scrc_acc, scrc_time, scrc_params, scrc_model = train_scrc(epochs=5)
    print(f"\n  âœ… SCRC V2 æµ‹è¯•å‡†ç¡®ç‡: {scrc_acc:.2f}%")
    print(f"  â±ï¸  è€—æ—¶: {scrc_time:.1f}s | ğŸ“Š å‚æ•°: {scrc_params:,}")
    
    # 2. Attention å¯¹ç…§
    print("\n" + "-"*60)
    print("  [B] Attention å¯¹ç…§ç»„ï¼ˆå…¨ç¨‹ BPï¼‰")
    print("-"*60)
    attn_model = SimpleAttention(784, 200, 10).to(device)
    attn_acc, attn_time, attn_params = train_bp_model(attn_model, "Attn", 5)
    print(f"\n  âœ… Attention æµ‹è¯•å‡†ç¡®ç‡: {attn_acc:.2f}%")
    print(f"  â±ï¸  è€—æ—¶: {attn_time:.1f}s | ğŸ“Š å‚æ•°: {attn_params:,}")
    
    # 3. MLP åŸºçº¿
    print("\n" + "-"*60)
    print("  [C] MLP åŸºçº¿å¯¹ç…§ï¼ˆå…¨ç¨‹ BPï¼‰")
    print("-"*60)
    mlp_model = SimpleMLP().to(device)
    mlp_acc, mlp_time, mlp_params = train_bp_model(mlp_model, "MLP", 5)
    print(f"\n  âœ… MLP æµ‹è¯•å‡†ç¡®ç‡: {mlp_acc:.2f}%")
    print(f"  â±ï¸  è€—æ—¶: {mlp_time:.1f}s | ğŸ“Š å‚æ•°: {mlp_params:,}")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("  ğŸ† å®éªŒæ€»ç»“")
    print("="*70)
    print(f"  {'æ¨¡å‹':<22} {'å‡†ç¡®ç‡':>8} {'å‚æ•°é‡':>12} {'æ—¶é—´':>8} {'å­¦ä¹ æ–¹å¼'}")
    print(f"  {'-'*65}")
    print(f"  {'SCRC V2 (Hebbian)':<22} {scrc_acc:>7.2f}% {scrc_params:>11,} {scrc_time:>7.1f}s {'Hebbian+BPè¯»å‡º'}")
    print(f"  {'Attention (BP)':<22} {attn_acc:>7.2f}% {attn_params:>11,} {attn_time:>7.1f}s {'å…¨ç¨‹BP'}")
    print(f"  {'MLP (BP)':<22} {mlp_acc:>7.2f}% {mlp_params:>11,} {mlp_time:>7.1f}s {'å…¨ç¨‹BP'}")
    
    # ä¿å­˜
    results = {
        'scrc_v2': {'acc': scrc_acc, 'time': scrc_time, 'params': scrc_params},
        'attention': {'acc': attn_acc, 'time': attn_time, 'params': attn_params},
        'mlp': {'acc': mlp_acc, 'time': mlp_time, 'params': mlp_params},
    }
    with open('tempdata/scrc_v2_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\n  ğŸ“ ç»“æœå·²ä¿å­˜åˆ° tempdata/scrc_v2_results.json")
