# -*- coding: utf-8 -*-
"""
================================================================================
SCRC éªŒè¯å®éªŒï¼šç¨€ç–ç«äº‰å¾ªç¯å›è·¯ vs Attention
================================================================================
å®éªŒç›®æ ‡ï¼š
  1. å®ç° SCRCï¼ˆSparse Competitive Recurrent Circuitï¼‰æ ¸å¿ƒç»“æ„
  2. å®éªŒä¸€ï¼šSCRC vs Attention åœ¨ MNIST ä¸Šçš„ç‰¹å¾æ‹Ÿåˆèƒ½åŠ›å¯¹æ¯”
  3. å®éªŒäºŒï¼šSCRC å¤šçº§ä¸²è”æ—¶æ˜¯å¦è‡ªåŠ¨æ¶Œç°å±‚æ¬¡åŒ–ç‰¹å¾

æ ¸å¿ƒå…¬å¼ï¼š
  Z = top_k(W Â· X)
  Î”W = Î· Â· Z Â· X^T    (Hebbian Learning)
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time
import os
import json

# ============================================================
# ç¡®ä¿ GPU
# ============================================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"[è®¾å¤‡] ä½¿ç”¨: {device}")
if device.type == 'cuda':
    print(f"[GPU] {torch.cuda.get_device_name(0)}, æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# ============================================================
# æ•°æ®åŠ è½½
# ============================================================
print("\n[æ•°æ®] åŠ è½½ MNIST ...")
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))  # å±•å¹³ä¸º 784 ç»´
])
train_dataset = datasets.MNIST('./tempdata', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./tempdata', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
print(f"[æ•°æ®] è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")


# ============================================================
# æ ¸å¿ƒç»“æ„ä¸€ï¼šSCRC â€” ç¨€ç–ç«äº‰å¾ªç¯å›è·¯
# ============================================================
class SCRC(nn.Module):
    """
    ç¨€ç–ç«äº‰å¾ªç¯å›è·¯ (Sparse Competitive Recurrent Circuit)
    
    è¿™æ˜¯æˆ‘ä»¬æå‡ºçš„å¤§è„‘æ ¸å¿ƒè®¡ç®—åŸè¯­ï¼š
      Z = top_k(W Â· X)           -- å…´å¥‹æŠ•å°„ + ç«äº‰æŠ‘åˆ¶
      Î”W = Î· Â· Z Â· X^T           -- Hebbian å¯å¡‘æ€§
    
    ä¸‰ä¸ªç»„åˆ†ï¼š
      E (å…´å¥‹æŠ•å°„)ï¼šW Â· Xï¼Œæ¯ä¸ªç¥ç»å…ƒçš„çªè§¦æƒé‡ä¸è¾“å…¥åšå†…ç§¯
      I (ç«äº‰æŠ‘åˆ¶)ï¼štop_kï¼Œåªä¿ç•™æœ€å¼ºçš„ k ä¸ªæ¿€æ´»
      P (Hebbian å¯å¡‘)ï¼šÎ”W = Î· Â· z Â· x^Tï¼ŒåŒæ—¶æ¿€æ´»åˆ™åŠ å¼ºè¿æ¥
    """
    def __init__(self, input_dim, num_units, sparsity_k, lr_hebbian=0.01):
        super().__init__()
        self.input_dim = input_dim
        self.num_units = num_units
        self.k = sparsity_k
        self.lr = lr_hebbian
        
        # Wï¼šç‰¹å¾æ¨¡æ¿çŸ©é˜µ [num_units, input_dim]
        # æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ª"ç‰¹å¾æ£€æµ‹å™¨"
        self.W = nn.Parameter(
            torch.randn(num_units, input_dim, device=device) * 0.01,
            requires_grad=False  # ä¸ç”¨æ¢¯åº¦ï¼ç”¨ Hebbianï¼
        )
        
    def forward(self, x):
        """
        x: [batch, input_dim]
        è¿”å›: [batch, num_units] ç¨€ç–æ¿€æ´»
        """
        # å…´å¥‹æŠ•å°„ï¼šæ¯ä¸ªç¥ç»å…ƒä¸è¾“å…¥åšå†…ç§¯
        scores = x @ self.W.T  # [batch, num_units]
        
        # ç«äº‰æŠ‘åˆ¶ï¼šåªä¿ç•™ Top-K æœ€å¼ºçš„
        topk_vals, topk_idx = torch.topk(scores, self.k, dim=1)
        
        # æ„å»ºç¨€ç–è¾“å‡º
        z = torch.zeros_like(scores)
        z.scatter_(1, topk_idx, F.relu(topk_vals))  # ReLU ç¡®ä¿éè´Ÿ
        
        return z
    
    def hebbian_update(self, x, z):
        """
        Hebbian å­¦ä¹ ï¼šåŒæ—¶æ¿€æ´»åˆ™è¿æ¥åŠ å¼º
        Î”W = Î· Â· (z^T Â· x) / batch_size
        
        åŠ å…¥æƒé‡å½’ä¸€åŒ–é˜²æ­¢çˆ†ç‚¸
        """
        batch_size = x.shape[0]
        # å¤–ç§¯æ›´æ–°
        delta_W = (z.T @ x) / batch_size  # [num_units, input_dim]
        self.W.data += self.lr * delta_W
        
        # L2 å½’ä¸€åŒ–æ¯ä¸€è¡Œï¼ˆä¿æŒæ¨¡æ¿çš„æ–¹å‘ï¼Œæ§åˆ¶å¹…å€¼ï¼‰
        norms = self.W.data.norm(dim=1, keepdim=True).clamp(min=1e-8)
        self.W.data = self.W.data / norms


# ============================================================
# æ ¸å¿ƒç»“æ„äºŒï¼šç®€å• Attention å¯¹ç…§ç»„
# ============================================================
class SimpleAttention(nn.Module):
    """
    æ ‡å‡†è‡ªæ³¨æ„åŠ› + åˆ†ç±»å¤´ï¼Œç”¨äºå…¬å¹³å¯¹æ¯”ã€‚
    ä½¿ç”¨åå‘ä¼ æ’­è®­ç»ƒã€‚
    """
    def __init__(self, input_dim, hidden_dim, num_classes, num_heads=4):
        super().__init__()
        self.embed = nn.Linear(input_dim, hidden_dim)
        self.W_Q = nn.Linear(hidden_dim, hidden_dim)
        self.W_K = nn.Linear(hidden_dim, hidden_dim)
        self.W_V = nn.Linear(hidden_dim, hidden_dim)
        self.head = nn.Linear(hidden_dim, num_classes)
        self.hidden_dim = hidden_dim
        
    def forward(self, x):
        h = F.relu(self.embed(x))  # [batch, hidden]
        Q = self.W_Q(h)
        K = self.W_K(h)
        V = self.W_V(h)
        # è‡ªæ³¨æ„åŠ›ï¼ˆå•ä»¤ç‰Œæƒ…å†µä¸‹ç­‰ä»·äºåŠ æƒæŠ•å°„ï¼‰
        attn = F.softmax(Q * K / (self.hidden_dim ** 0.5), dim=-1)
        out = attn * V
        return self.head(out)


# ============================================================
# SCRC åˆ†ç±»å™¨ï¼ˆå¤šçº§ SCRC + ç®€å•çº¿æ€§è¯»å‡ºï¼‰
# ============================================================
class SCRCClassifier(nn.Module):
    """
    å¤šçº§ SCRC ä¸²è” + çº¿æ€§è¯»å‡ºå¤´
    
    éªŒè¯ï¼šå±‚æ¬¡ç‰¹å¾æ˜¯å¦è‡ªåŠ¨æ¶Œç°
    """
    def __init__(self, dims, ks, lr_hebbian=0.01):
        """
        dims: [input_dim, layer1_units, layer2_units, ...]
        ks:   [k1, k2, ...]  æ¯ä¸€çº§çš„ç¨€ç–åº¦
        """
        super().__init__()
        self.layers = nn.ModuleList()
        for i in range(len(dims) - 1):
            self.layers.append(SCRC(dims[i], dims[i+1], ks[i], lr_hebbian))
        
        # çº¿æ€§è¯»å‡ºå¤´ï¼ˆè¿™ä¸ªç”¨æ¢¯åº¦è®­ç»ƒï¼Œå› ä¸ºå®ƒåªæ˜¯ä¸€ä¸ªæ ‡ç­¾æ˜ å°„ï¼‰
        self.readout = nn.Linear(dims[-1], 10).to(device)
        
    def forward(self, x, learn=False):
        activations = [x]
        for layer in self.layers:
            x = layer(x)
            activations.append(x)
            
        logits = self.readout(x)
        
        if learn:
            # Hebbian æ›´æ–°æ¯ä¸€å±‚
            for i, layer in enumerate(self.layers):
                layer.hebbian_update(activations[i], activations[i+1])
        
        return logits, activations


# ============================================================
# å®éªŒä¸€ï¼šSCRC vs Attention ç‰¹å¾æ‹Ÿåˆå¯¹æ¯”
# ============================================================
def experiment_1_comparison():
    print("\n" + "="*70)
    print("  å®éªŒä¸€ï¼šSCRC vs Attention â€” MNIST ç‰¹å¾æ‹Ÿåˆèƒ½åŠ›å¯¹æ¯”")
    print("="*70)
    
    results = {}
    
    # ---------- SCRC æ–¹æ¡ˆ ----------
    print("\n--- [A] SCRC æ–¹æ¡ˆ ---")
    print("  ç»“æ„: 784 â†’ 500(k=25) â†’ 200(k=10) â†’ çº¿æ€§è¯»å‡º(10)")
    print("  å­¦ä¹ : SCRC å±‚ç”¨ Hebbianï¼Œè¯»å‡ºå¤´ç”¨æ¢¯åº¦ä¸‹é™")
    
    scrc_model = SCRCClassifier(
        dims=[784, 500, 200],
        ks=[25, 10],
        lr_hebbian=0.005
    ).to(device)
    
    # è¯»å‡ºå¤´çš„ä¼˜åŒ–å™¨
    readout_optimizer = torch.optim.Adam(scrc_model.readout.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    scrc_params = sum(p.numel() for p in scrc_model.parameters())
    print(f"  æ€»å‚æ•°é‡: {scrc_params:,}")
    
    # è®­ç»ƒ
    scrc_train_start = time.time()
    for epoch in range(5):
        correct = 0
        total = 0
        epoch_loss = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            logits, _ = scrc_model(data, learn=True)  # Hebbian æ›´æ–°
            
            loss = criterion(logits, target)
            readout_optimizer.zero_grad()
            loss.backward()
            readout_optimizer.step()
            
            pred = logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
            epoch_loss += loss.item()
            
        acc = correct / total * 100
        avg_loss = epoch_loss / len(train_loader)
        print(f"  Epoch {epoch+1}/5 | æŸå¤±: {avg_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {acc:.2f}%")
    
    scrc_train_time = time.time() - scrc_train_start
    
    # æµ‹è¯•
    scrc_model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits, _ = scrc_model(data, learn=False)
            pred = logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    scrc_test_acc = correct / total * 100
    print(f"\n  âœ… SCRC æµ‹è¯•å‡†ç¡®ç‡: {scrc_test_acc:.2f}%")
    print(f"  â±ï¸  è®­ç»ƒè€—æ—¶: {scrc_train_time:.2f}s")
    print(f"  ğŸ“Š å‚æ•°é‡: {scrc_params:,}")
    
    # æ£€æŸ¥ç¨€ç–æ€§
    with torch.no_grad():
        sample = next(iter(test_loader))[0][:1].to(device)
        _, acts = scrc_model(sample, learn=False)
        for i, act in enumerate(acts[1:]):
            sparsity = (act == 0).float().mean().item() * 100
            print(f"  ğŸ”¬ ç¬¬{i+1}å±‚ç¨€ç–ç‡: {sparsity:.1f}%")
    
    results['scrc'] = {
        'test_acc': scrc_test_acc,
        'train_time': scrc_train_time,
        'params': scrc_params,
    }
    
    # ---------- Attention æ–¹æ¡ˆ ----------
    print("\n--- [B] Attention å¯¹ç…§ç»„ ---")
    print("  ç»“æ„: 784 â†’ 200(Attention) â†’ çº¿æ€§è¯»å‡º(10)")
    print("  å­¦ä¹ : å…¨ç¨‹åå‘ä¼ æ’­")
    
    attn_model = SimpleAttention(784, 200, 10).to(device)
    attn_params = sum(p.numel() for p in attn_model.parameters())
    print(f"  æ€»å‚æ•°é‡: {attn_params:,}")
    
    attn_optimizer = torch.optim.Adam(attn_model.parameters(), lr=0.001)
    
    attn_train_start = time.time()
    for epoch in range(5):
        correct = 0
        total = 0
        epoch_loss = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            logits = attn_model(data)
            loss = criterion(logits, target)
            
            attn_optimizer.zero_grad()
            loss.backward()
            attn_optimizer.step()
            
            pred = logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
            epoch_loss += loss.item()
            
        acc = correct / total * 100
        avg_loss = epoch_loss / len(train_loader)
        print(f"  Epoch {epoch+1}/5 | æŸå¤±: {avg_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {acc:.2f}%")
    
    attn_train_time = time.time() - attn_train_start
    
    # æµ‹è¯•
    attn_model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits = attn_model(data)
            pred = logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    attn_test_acc = correct / total * 100
    print(f"\n  âœ… Attention æµ‹è¯•å‡†ç¡®ç‡: {attn_test_acc:.2f}%")
    print(f"  â±ï¸  è®­ç»ƒè€—æ—¶: {attn_train_time:.2f}s")
    print(f"  ğŸ“Š å‚æ•°é‡: {attn_params:,}")
    
    results['attention'] = {
        'test_acc': attn_test_acc,
        'train_time': attn_train_time,
        'params': attn_params,
    }
    
    # ---------- æ€»ç»“å¯¹æ¯” ----------
    print("\n" + "="*70)
    print("  å®éªŒä¸€æ€»ç»“ï¼šSCRC vs Attention")
    print("="*70)
    print(f"  {'æŒ‡æ ‡':<20} {'SCRC':<20} {'Attention':<20}")
    print(f"  {'-'*60}")
    print(f"  {'æµ‹è¯•å‡†ç¡®ç‡':<18} {scrc_test_acc:.2f}%{'':<14} {attn_test_acc:.2f}%")
    print(f"  {'è®­ç»ƒæ—¶é—´':<19} {scrc_train_time:.2f}s{'':<14} {attn_train_time:.2f}s")
    print(f"  {'å‚æ•°é‡':<20} {scrc_params:<20,} {attn_params:<20,}")
    print(f"  {'å­¦ä¹ æ–¹å¼':<19} {'Hebbian(å±€éƒ¨)':<20} {'BP(å…¨å±€æ¢¯åº¦)'}")
    print(f"  {'ç¨€ç–æ€§':<20} {'âœ… Top-K ç¡¬ç¨€ç–':<20} {'âŒ å¯†é›†æ¿€æ´»'}")
    
    return results, scrc_model


# ============================================================
# å®éªŒäºŒï¼šSCRC å±‚æ¬¡æ¶Œç°åˆ†æ
# ============================================================
def experiment_2_hierarchy(scrc_model):
    print("\n" + "="*70)
    print("  å®éªŒäºŒï¼šSCRC å±‚æ¬¡æ¶Œç° â€” æ¯ä¸€çº§å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ")
    print("="*70)
    
    for i, layer in enumerate(scrc_model.layers):
        W = layer.W.data.cpu()
        
        # åˆ†ææƒé‡æ¨¡æ¿çš„ç»Ÿè®¡ç‰¹æ€§
        print(f"\n--- ç¬¬ {i+1} å±‚ (è¾“å…¥:{layer.input_dim} â†’ å•å…ƒ:{layer.num_units}, k={layer.k}) ---")
        
        # 1. æƒé‡çš„å¹³å‡éé›¶ç‡ï¼ˆæ¨¡æ¿çš„"å¤æ‚åº¦"ï¼‰
        nonzero_rate = (W.abs() > 0.01).float().mean().item() * 100
        print(f"  æ´»è·ƒæƒé‡å æ¯”: {nonzero_rate:.1f}%")
        
        # 2. æ¨¡æ¿é—´çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå¤šæ ·æ€§ï¼‰
        W_norm = F.normalize(W, dim=1)
        sim_matrix = W_norm @ W_norm.T
        # å»æ‰å¯¹è§’çº¿
        mask = ~torch.eye(sim_matrix.size(0), dtype=torch.bool)
        avg_sim = sim_matrix[mask].mean().item()
        max_sim = sim_matrix[mask].max().item()
        print(f"  æ¨¡æ¿é—´å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {avg_sim:.4f} (è¶Šä½è¶Šå¤šæ ·)")
        print(f"  æ¨¡æ¿é—´æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦: {max_sim:.4f}")
        
        # 3. æƒé‡çš„æœ‰æ•ˆç»´åº¦ï¼ˆPCA æ–¹å·®è§£é‡Šç‡ï¼‰
        try:
            U, S, V = torch.linalg.svd(W, full_matrices=False)
            explained = (S ** 2).cumsum(0) / (S ** 2).sum()
            eff_dim_90 = (explained < 0.9).sum().item() + 1
            eff_dim_99 = (explained < 0.99).sum().item() + 1
            print(f"  æœ‰æ•ˆç»´åº¦ (90%æ–¹å·®): {eff_dim_90} / {min(W.shape)}")
            print(f"  æœ‰æ•ˆç»´åº¦ (99%æ–¹å·®): {eff_dim_99} / {min(W.shape)}")
        except Exception:
            print(f"  SVD åˆ†æè·³è¿‡")
        
        # 4. ç¬¬ä¸€å±‚çš„å¯è§†åŒ–åˆ†æï¼ˆå¦‚æœæ˜¯ 784 ç»´è¾“å…¥ = 28x28 å›¾åƒï¼‰
        if layer.input_dim == 784:
            # æ‰¾å‡ºæœ€æ´»è·ƒçš„ 10 ä¸ªæ¨¡æ¿
            activation_strength = W.norm(dim=1)
            top10_idx = activation_strength.topk(10).indices
            
            print(f"\n  ğŸ“Š æœ€å¼º10ä¸ªç‰¹å¾æ£€æµ‹å™¨çš„æ¨¡å¼ç±»å‹åˆ†æ:")
            for rank, idx in enumerate(top10_idx):
                template = W[idx].reshape(28, 28)
                # åˆ†ææ¨¡æ¿çš„ç©ºé—´é¢‘ç‡
                high_freq = (template[:-1, :] - template[1:, :]).abs().mean() + \
                           (template[:, :-1] - template[:, 1:]).abs().mean()
                spatial_std = template.std()
                peak_loc = template.abs().argmax().item()
                peak_y, peak_x = peak_loc // 28, peak_loc % 28
                
                pattern_type = "è¾¹ç¼˜/çº¹ç†" if high_freq > spatial_std * 2 else "å—çŠ¶/åŒºåŸŸ"
                print(f"    #{rank+1} å•å…ƒ{idx.item():3d}: "
                      f"ç±»å‹={pattern_type}, "
                      f"å³°å€¼ä½ç½®=({peak_y},{peak_x}), "
                      f"ç©ºé—´é¢‘ç‡={high_freq:.4f}")


# ============================================================
# å®éªŒä¸‰ï¼šSCRC ä¸€æ¬¡æ€§å­¦ä¹ ï¼ˆOne-shotï¼‰vs Attention å¤šè½®è®­ç»ƒ
# ============================================================
def experiment_3_oneshot():
    print("\n" + "="*70)
    print("  å®éªŒä¸‰ï¼šSCRC ä¸€æ¬¡æ€§å­¦ä¹  â€” åªçœ‹ä¸€éæ•°æ®èƒ½å­¦åˆ°å¤šå°‘ï¼Ÿ")
    print("="*70)
    
    # SCRC åªè¿‡ä¸€éè®­ç»ƒé›†
    model = SCRCClassifier(
        dims=[784, 500, 200],
        ks=[25, 10],
        lr_hebbian=0.01
    ).to(device)
    
    readout_opt = torch.optim.Adam(model.readout.parameters(), lr=0.003)
    criterion = nn.CrossEntropyLoss()
    
    print("\n  [åªè®­ç»ƒ 1 ä¸ª epoch â€” æ¯ä¸ªæ ·æœ¬åªçœ‹ä¸€æ¬¡]")
    t0 = time.time()
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        logits, _ = model(data, learn=True)
        
        loss = criterion(logits, target)
        readout_opt.zero_grad()
        loss.backward()
        readout_opt.step()
        
        pred = logits.argmax(dim=1)
        correct += (pred == target).sum().item()
        total += target.size(0)
        
        if (batch_idx + 1) % 50 == 0:
            print(f"    è¿›åº¦: {batch_idx+1}/{len(train_loader)} | å½“å‰å‡†ç¡®ç‡: {correct/total*100:.2f}%")
    
    oneshot_time = time.time() - t0
    
    # æµ‹è¯•
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits, _ = model(data, learn=False)
            pred = logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    oneshot_acc = correct / total * 100
    print(f"\n  âœ… SCRC ä¸€æ¬¡æ€§å­¦ä¹ æµ‹è¯•å‡†ç¡®ç‡: {oneshot_acc:.2f}%")
    print(f"  â±ï¸  è®­ç»ƒè€—æ—¶: {oneshot_time:.2f}s (ä»…1ä¸ªepoch)")
    
    return oneshot_acc


# ============================================================
# ä¸»ç¨‹åº
# ============================================================
if __name__ == '__main__':
    print("="*70)
    print("  SCRC éªŒè¯å®éªŒï¼šç¨€ç–ç«äº‰å¾ªç¯å›è·¯ vs Attention")
    print("  Smart Competitive Recurrent Circuit Verification")
    print("="*70)
    print(f"  æ ¸å¿ƒå…¬å¼: Z = top_k(W Â· X), Î”W = Î· Â· Z Â· Xáµ€")
    print(f"  è®¾å¤‡: {device}")
    print()
    
    # å®éªŒä¸€ï¼šå¯¹æ¯”
    results, scrc_model = experiment_1_comparison()
    
    # å®éªŒäºŒï¼šå±‚æ¬¡æ¶Œç°
    experiment_2_hierarchy(scrc_model)
    
    # å®éªŒä¸‰ï¼šä¸€æ¬¡æ€§å­¦ä¹ 
    oneshot_acc = experiment_3_oneshot()
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("  ğŸ† å…¨éƒ¨å®éªŒå®Œæˆ â€” æœ€ç»ˆæ€»ç»“")
    print("="*70)
    print(f"  SCRC (5 epochs)    â†’ å‡†ç¡®ç‡: {results['scrc']['test_acc']:.2f}%, "
          f"å‚æ•°: {results['scrc']['params']:,}, "
          f"æ—¶é—´: {results['scrc']['train_time']:.1f}s")
    print(f"  Attention (5 epochs) â†’ å‡†ç¡®ç‡: {results['attention']['test_acc']:.2f}%, "
          f"å‚æ•°: {results['attention']['params']:,}, "
          f"æ—¶é—´: {results['attention']['train_time']:.1f}s")
    print(f"  SCRC (1 epoch ä¸€æ¬¡æ€§) â†’ å‡†ç¡®ç‡: {oneshot_acc:.2f}%")
    print()
    
    # ä¿å­˜ç»“æœ
    os.makedirs('tempdata', exist_ok=True)
    final_results = {
        'scrc_5epoch': results['scrc'],
        'attention_5epoch': results['attention'],
        'scrc_oneshot_acc': oneshot_acc,
    }
    with open('tempdata/scrc_experiment_results.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    print("  ğŸ“ ç»“æœå·²ä¿å­˜åˆ° tempdata/scrc_experiment_results.json")
