# -*- coding: utf-8 -*-
"""
================================================================================
SCRC V3: ç»Ÿä¸€ç»“æ„â€”â€”å­¦ä¹ ä¸æ¨ç†æ˜¯åŒä¸€å›è·¯çš„ä¸¤ç§æ¨¡å¼
================================================================================

æ ¸å¿ƒæ´å¯Ÿï¼š
  å­¦ä¹ å’Œæ¨ç†ä¸æ˜¯ä¸¤å¥—ç‹¬ç«‹ç³»ç»Ÿï¼Œè€Œæ˜¯åŒä¸€ä¸ªç¥ç»å›è·¯çš„ä¸¤ç§è¿è¡Œæ¨¡å¼ã€‚

ç»Ÿä¸€å›è·¯ï¼š
  å‰å‘ï¼ˆæ¨ç†æ¨¡å¼ï¼‰: z = top_k(W Â· x)          â† ç¨€ç–ç‰¹å¾åŒ¹é…
  åå‘ï¼ˆå­¦ä¹ æ¨¡å¼ï¼‰: xÌ‚ = W^T Â· z              â† ç”¨ç¨€ç–ç é‡å»ºè¾“å…¥
                    e = x - xÌ‚                â† é¢„æµ‹è¯¯å·®ï¼ˆå±€éƒ¨è®¡ç®—ï¼ï¼‰
                    Î”W = Î· Â· z Â· e^T          â† è¯¯å·®é©±åŠ¨çš„å±€éƒ¨å­¦ä¹ 

è¿™å°±æ˜¯"é¢„æµ‹ç¼–ç  + ç¨€ç–ç¼–ç "ï¼š
  - æ¨ç†æ—¶ï¼šWÂ·x ç„¶å top-kï¼ˆå¿«é€Ÿã€ç¨€ç–ã€ä½èƒ½è€—ï¼‰
  - å­¦ä¹ æ—¶ï¼šåŒä¸€ä¸ª W çš„è½¬ç½®åšé‡å»ºï¼Œè¯¯å·®åä¼ ï¼ˆå±€éƒ¨ã€æ— éœ€å…¨å±€BPï¼‰

å…³é”®åŒºåˆ« vs çº¯ Hebbianï¼š
  Hebbian:  Î”W = Î· Â· z Â· x^T       â† åªçœ‹"è¾“å…¥æ˜¯ä»€ä¹ˆ"
  é¢„æµ‹ç¼–ç : Î”W = Î· Â· z Â· (x-W^Tz)^T â† çœ‹"æˆ‘æ²¡å­¦åˆ°çš„æ˜¯ä»€ä¹ˆ"ï¼ˆè¯¯å·®é©±åŠ¨ï¼‰

è¿™å°±æ˜¯åŒä¸€ä¸ªç»“æ„çš„ä¸¤ç§æ¨¡å¼ï¼š
  - W åšå‰å‘æŠ•å°„ï¼ˆæ¨ç†ï¼‰
  - W^T åšåå‘é‡å»ºï¼ˆå­¦ä¹ æ—¶ç”Ÿæˆé¢„æµ‹ï¼‰
  - è¯¯å·® e æ˜¯å±€éƒ¨å¯è®¡ç®—çš„ï¼Œä¸éœ€è¦å…¨å±€æ¢¯åº¦
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time
import json
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"[è®¾å¤‡] ä½¿ç”¨: {device}")
if device.type == 'cuda':
    print(f"[GPU] {torch.cuda.get_device_name(0)}")


# æ•°æ®
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST('./tempdata', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./tempdata', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)


class UnifiedCircuit(nn.Module):
    """
    ç»Ÿä¸€ç¥ç»å›è·¯ï¼šå­¦ä¹ å’Œæ¨ç†æ˜¯åŒä¸€ç»“æ„çš„ä¸¤ç§æ¨¡å¼
    
    ç»“æ„ï¼ˆåªæœ‰ä¸€ä¸ªçŸ©é˜µ Wï¼‰ï¼š
    
    æ¨ç†æ¨¡å¼ï¼ˆå‰å‘ï¼‰ï¼š
      scores = W Â· x       â† å†…ç§¯åŒ¹é…
      z = top_k(scores)    â† ç«äº‰ç¨€ç–
      
    å­¦ä¹ æ¨¡å¼ï¼ˆå‰å‘+åå‘ï¼‰ï¼š
      scores = W Â· x       â† åŒä¸€ä¸ªå‰å‘
      z = top_k(scores)    â† åŒä¸€ä¸ªç«äº‰
      xÌ‚ = W^T Â· z         â† W çš„è½¬ç½®åšé‡å»ºï¼ˆä¸æ˜¯æ–°å‚æ•°ï¼ï¼‰
      e = x - xÌ‚           â† é¢„æµ‹è¯¯å·®ï¼ˆçº¯å±€éƒ¨ï¼ï¼‰
      Î”W = Î· Â· z Â· e^T     â† ç”¨è¯¯å·®è€ŒéåŸå§‹è¾“å…¥æ›´æ–°
    
    å…³é”®ï¼šW å’Œ W^T æ˜¯åŒä¸€ä¸ªçŸ©é˜µï¼Œä¸æ˜¯ä¸¤ä¸ªç‹¬ç«‹å‚æ•°ã€‚
    å‰å‘ç”¨ Wï¼Œåå‘é‡å»ºç”¨ W^Tã€‚è¿™å°±æ˜¯"åŒä¸€ç»“æ„çš„ä¸¤ç§æ¨¡å¼"ã€‚
    """
    def __init__(self, input_dim, num_units, k, lr=0.01):
        super().__init__()
        self.k = k
        self.lr = lr
        self.num_units = num_units
        self.input_dim = input_dim
        
        # å”¯ä¸€çš„å‚æ•°ï¼šç‰¹å¾å­—å…¸ W
        W = torch.randn(num_units, input_dim, device=device) * 0.01
        W = F.normalize(W, dim=1)
        self.W = nn.Parameter(W, requires_grad=False)
        
    def forward(self, x, mode='infer'):
        """
        mode='infer': åªåšå‰å‘æ¨ç†ï¼ˆå¿«é€Ÿã€ä½èƒ½è€—ï¼‰
        mode='learn': å‰å‘ + åå‘é‡å»º + è¯¯å·®é©±åŠ¨æ›´æ–°
        """
        # ===== å‰å‘ï¼ˆæ¨ç†æ¨¡å¼ï¼‰=====
        x_norm = F.normalize(x, dim=1)
        scores = x_norm @ self.W.T  # [batch, num_units]
        
        # Top-K ç«äº‰æŠ‘åˆ¶
        topk_vals, topk_idx = torch.topk(scores, self.k, dim=1)
        z = torch.zeros_like(scores)
        z.scatter_(1, topk_idx, topk_vals.clamp(min=0))
        
        if mode == 'learn':
            # ===== åå‘ï¼ˆå­¦ä¹ æ¨¡å¼ï¼‰=====
            # ç”¨åŒä¸€ä¸ª W çš„è½¬ç½®é‡å»ºè¾“å…¥
            x_hat = z @ self.W  # [batch, input_dim]  â† W^T Â· z
            
            # é¢„æµ‹è¯¯å·®ï¼ˆå±€éƒ¨è®¡ç®—ï¼Œæ— éœ€å…¨å±€æ¢¯åº¦ï¼ï¼‰
            error = x_norm - x_hat  # [batch, input_dim]
            
            # è¯¯å·®é©±åŠ¨çš„å­¦ä¹ 
            # Î”W = Î· Â· z^T Â· error / batch_size
            batch_size = x.shape[0]
            delta_W = (z.T @ error) / batch_size  # [num_units, input_dim]
            self.W.data += self.lr * delta_W
            
            # å½’ä¸€åŒ–ä¿æŒç¨³å®š
            self.W.data = F.normalize(self.W.data, dim=1)
        
        return z


class UnifiedClassifier(nn.Module):
    """å¤šçº§ç»Ÿä¸€å›è·¯ + çº¿æ€§è¯»å‡º"""
    def __init__(self):
        super().__init__()
        self.layer1 = UnifiedCircuit(784, 500, k=50, lr=0.1)
        self.layer2 = UnifiedCircuit(500, 200, k=20, lr=0.1)
        self.readout = nn.Linear(200, 10).to(device)
        
    def forward(self, x, mode='infer'):
        z1 = self.layer1(x, mode=mode)
        z2 = self.layer2(z1, mode=mode)
        logits = self.readout(z2)
        return logits, z1, z2


class SimpleMLP(nn.Module):
    """MLP å¯¹ç…§ç»„"""
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 500)
        self.fc2 = nn.Linear(500, 200)
        self.fc3 = nn.Linear(200, 10)
    def forward(self, x):
        return self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))


def train_unified(epochs=10):
    """è®­ç»ƒç»Ÿä¸€å›è·¯"""
    model = UnifiedClassifier().to(device)
    # åªæœ‰è¯»å‡ºå¤´ç”¨æ¢¯åº¦
    optimizer = torch.optim.Adam(model.readout.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    params = sum(p.numel() for p in model.parameters())
    
    print(f"  å‚æ•°é‡: {params:,}")
    print(f"  ç»“æ„: 784â†’500(k=50)â†’200(k=20)â†’10")
    print(f"  ç‰¹å¾å­¦ä¹ : é¢„æµ‹ç¼–ç ï¼ˆå±€éƒ¨è¯¯å·®é©±åŠ¨ï¼‰")
    print(f"  è¯»å‡ºå­¦ä¹ : Adamï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰")
    
    t0 = time.time()
    for epoch in range(epochs):
        correct = total = 0
        loss_sum = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # å­¦ä¹ æ¨¡å¼ï¼šå‰å‘+åå‘é‡å»º+è¯¯å·®æ›´æ–°
            logits, z1, z2 = model(data, mode='learn')
            
            loss = criterion(logits, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
            loss_sum += loss.item()
        
        acc = correct / total * 100
        avg_loss = loss_sum / len(train_loader)
        
        # è®¡ç®—é‡å»ºè´¨é‡
        with torch.no_grad():
            sample = data[:100]
            sample_norm = F.normalize(sample, dim=1)
            z1_test = model.layer1(sample, mode='infer')
            recon = z1_test @ model.layer1.W
            recon_error = (sample_norm - recon).pow(2).mean().item()
        
        print(f"  Epoch {epoch+1:2d}/{epochs} | æŸå¤±: {avg_loss:.4f} | "
              f"å‡†ç¡®ç‡: {acc:.2f}% | é‡å»ºè¯¯å·®: {recon_error:.4f}")
    
    train_time = time.time() - t0
    
    # æµ‹è¯•ï¼ˆçº¯æ¨ç†æ¨¡å¼â€”â€”ä¸æ›´æ–°æƒé‡ï¼‰
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            logits, _, _ = model(data, mode='infer')  # æ¨ç†æ¨¡å¼ï¼
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
    test_acc = correct / total * 100
    
    # ç¨€ç–æ€§
    with torch.no_grad():
        sample_data = next(iter(test_loader))[0].to(device)
        _, z1, z2 = model(sample_data, mode='infer')
        sp1 = (z1 == 0).float().mean().item() * 100
        sp2 = (z2 == 0).float().mean().item() * 100
    
    # æ¨¡æ¿å¤šæ ·æ€§
    W1 = model.layer1.W.data
    W1n = F.normalize(W1, dim=1)
    sim = (W1n @ W1n.T)
    mask = ~torch.eye(sim.size(0), dtype=torch.bool, device=device)
    avg_sim = sim[mask].mean().item()
    
    print(f"\n  âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"  â±ï¸  è®­ç»ƒæ—¶é—´: {train_time:.1f}s")
    print(f"  ğŸ”¬ ç¨€ç–æ€§: L1={sp1:.1f}%, L2={sp2:.1f}%")
    print(f"  ğŸ“Š æ¨¡æ¿å¤šæ ·æ€§: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦={avg_sim:.4f}")
    
    return test_acc, train_time, params, model


def train_mlp(epochs=10):
    """MLP å¯¹ç…§ç»„"""
    model = SimpleMLP().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    params = sum(p.numel() for p in model.parameters())
    
    t0 = time.time()
    for epoch in range(epochs):
        correct = total = 0
        loss_sum = 0
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            logits = model(data)
            loss = criterion(logits, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            correct += (logits.argmax(1) == target).sum().item()
            total += target.size(0)
            loss_sum += loss.item()
        print(f"  Epoch {epoch+1:2d}/{epochs} | æŸå¤±: {loss_sum/len(train_loader):.4f} | å‡†ç¡®ç‡: {correct/total*100:.2f}%")
    
    train_time = time.time() - t0
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            correct += (model(data).argmax(1) == target).sum().item()
            total += target.size(0)
    test_acc = correct / total * 100
    print(f"\n  âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"  â±ï¸  è®­ç»ƒæ—¶é—´: {train_time:.1f}s")
    return test_acc, train_time, params


if __name__ == '__main__':
    print("="*70)
    print("  SCRC V3: ç»Ÿä¸€å›è·¯â€”â€”å­¦ä¹ ä¸æ¨ç†æ˜¯åŒä¸€ç»“æ„çš„ä¸¤ç§æ¨¡å¼")
    print("="*70)
    print()
    print("  å‰å‘ = æ¨ç†: z = top_k(W Â· x)")
    print("  åå‘ = å­¦ä¹ : e = x - W^TÂ·z, Î”W = Î·Â·zÂ·e^T")
    print("  å…³é”®: W å’Œ W^T æ˜¯åŒä¸€ä¸ªçŸ©é˜µçš„ä¸¤ä¸ªæ–¹å‘")
    print()
    
    print("-"*60)
    print("  [A] ç»Ÿä¸€å›è·¯ï¼ˆé¢„æµ‹ç¼–ç  + Top-K ç¨€ç–ï¼‰")
    print("-"*60)
    unified_acc, unified_time, unified_params, unified_model = train_unified(10)
    
    print()
    print("-"*60)
    print("  [B] MLP å¯¹ç…§ç»„ï¼ˆå…¨ç¨‹åå‘ä¼ æ’­ï¼‰")
    print("-"*60)
    mlp_acc, mlp_time, mlp_params = train_mlp(10)
    
    # æ€»ç»“
    print()
    print("="*70)
    print("  ğŸ† å®éªŒæ€»ç»“ï¼šç»Ÿä¸€å›è·¯ vs åå‘ä¼ æ’­")
    print("="*70)
    print(f"  {'æ¨¡å‹':<25} {'å‡†ç¡®ç‡':>8} {'å‚æ•°é‡':>12} {'æ—¶é—´':>8} {'å­¦ä¹ æ–¹å¼'}")
    print(f"  {'-'*70}")
    print(f"  {'ç»Ÿä¸€å›è·¯(é¢„æµ‹ç¼–ç )':<23} {unified_acc:>7.2f}% {unified_params:>11,} {unified_time:>7.1f}s {'å±€éƒ¨è¯¯å·®+BPè¯»å‡º'}")
    print(f"  {'MLP(å…¨ç¨‹BP)':<24} {mlp_acc:>7.2f}% {mlp_params:>11,} {mlp_time:>7.1f}s {'å…¨å±€åå‘ä¼ æ’­'}")
    print()
    
    improvement_vs_v2 = unified_acc - 21.06
    print(f"  ğŸ“ˆ vs SCRC V2 (çº¯Hebbian 21.06%): {'+' if improvement_vs_v2 > 0 else ''}{improvement_vs_v2:.2f}%")
    print(f"  ğŸ“ˆ vs MLP: {unified_acc - mlp_acc:+.2f}%")
    
    results = {
        'unified_circuit': {'acc': unified_acc, 'time': unified_time, 'params': unified_params},
        'mlp_bp': {'acc': mlp_acc, 'time': mlp_time, 'params': mlp_params},
        'scrc_v2_reference': 21.06,
        'scrc_v1_reference': 17.65,
    }
    os.makedirs('tempdata', exist_ok=True)
    with open('tempdata/scrc_v3_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\n  ğŸ“ ç»“æœå·²ä¿å­˜åˆ° tempdata/scrc_v3_results.json")
